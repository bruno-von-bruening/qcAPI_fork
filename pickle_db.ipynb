{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import sqlite3\n",
    "from qcapi_utils import QCRecord, Conformation,get_record_id\n",
    "import requests\n",
    "\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"http://127.0.0.1:8000/list_records/?status=1\")\n",
    "records = response.json()\n",
    "print(len(records))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_numpy(record,keys=None):\n",
    "    output = dict(record[\"conformation\"])\n",
    "    if keys is not None:\n",
    "        output = {key:output[key] for key in keys}\n",
    "    for key in output:\n",
    "        if isinstance(output[key],list):\n",
    "            output[key] = np.asarray(output[key])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [\n",
    "    \"species\",\n",
    "    \"coordinates\",\n",
    "    \"total_charge\",\n",
    "    \"energy\",\n",
    "    \"forces\",\n",
    "    \"mbis_charges\",\n",
    "    \"mbis_volumes\",\n",
    "    # \"mbis_volume_ratios\",\n",
    "    \"mbis_valence_widths\",\n",
    "]\n",
    "dataset_raw = list(\n",
    "    map(\n",
    "        lambda x: convert_numpy(x, keys=keys),\n",
    "        records,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'energy': -0.4987605100487198, 'spin': 1, 'nel': 1, 'MBIS_volume': 1.2409718845631814, 'MBIS_sigma': 0.27227412413766605}\n",
      "[ 1  6  8 11 15]\n",
      "[   0.           -0.49876051    0.            0.            0.\n",
      "    0.          -37.87264504    0.          -75.11317841    0.\n",
      "    0.         -162.30553818    0.            0.            0.\n",
      " -341.3059197 ]\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "from client import PERIODIC_TABLE\n",
    "with open(\"atomic_data.yaml\",'r') as f:\n",
    "    atomic_data = yaml.safe_load(f)\n",
    "print(atomic_data[\"H_0\"])\n",
    "\n",
    "species_set = np.unique(np.concatenate([d[\"species\"] for d in dataset_raw]))\n",
    "print(species_set)\n",
    "atomic_energies = np.zeros(np.max(species_set)+1)\n",
    "for species in species_set:\n",
    "    atomic_energies[species] = atomic_data[PERIODIC_TABLE[species]+\"_0\"][\"energy\"]\n",
    "print(atomic_energies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "KCALPERMOL = 627.5096080305927\n",
    "\n",
    "dataset = []\n",
    "for d in dataset_raw:\n",
    "    species = d[\"species\"]\n",
    "    reference_energy = np.sum(atomic_energies[species])\n",
    "    formation_energy = d[\"energy\"] - reference_energy\n",
    "    coordinates = d[\"coordinates\"]\n",
    "    forces = d[\"forces\"]\n",
    "\n",
    "    virial_tensor = -(forces[:,:,None]*coordinates[:,None,:]).sum(axis=0,keepdims=True)\n",
    "\n",
    "    forces = forces*KCALPERMOL\n",
    "    virial_tensor = virial_tensor*KCALPERMOL\n",
    "    formation_energy = formation_energy*KCALPERMOL\n",
    "    energy = d[\"energy\"]*KCALPERMOL\n",
    "\n",
    "    d2 = {\n",
    "        **d,\n",
    "        \"formation_energy\":formation_energy,\n",
    "        \"energy\":energy,\n",
    "        \"forces\":forces,\n",
    "        \"virial_tensor\":virial_tensor,\n",
    "    }\n",
    "    dataset.append(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-82.45059772254784 1.281363516812011 -85.19286826041558 -75.50506062849448\n"
     ]
    }
   ],
   "source": [
    "energies = np.array([d[\"formation_energy\"] for d in dataset])\n",
    "natoms = np.array([d[\"species\"].shape[0] for d in dataset])\n",
    "eperatom = energies/natoms\n",
    "print(np.mean(eperatom),np.std(eperatom),np.min(eperatom),np.max(eperatom))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[159  14  80 163 114 139  64  68  17  35]\n",
      "train dataset size : 176\n",
      "validation dataset size : 2\n"
     ]
    }
   ],
   "source": [
    "validation_size = int(round(0.01*len(dataset)))\n",
    "train_size = len(dataset) - validation_size\n",
    "\n",
    "#get numpy generator\n",
    "rng = np.random.default_rng(202420032050)\n",
    "idx_shuffle = rng.permutation(len(dataset))\n",
    "print(idx_shuffle[:10])\n",
    "train_idx = idx_shuffle[:train_size]\n",
    "validation_idx = idx_shuffle[train_size:]\n",
    "\n",
    "train_dataset = [dataset[i] for i in train_idx]\n",
    "validation_dataset = [dataset[i] for i in validation_idx]\n",
    "print(f\"train dataset size : {len(train_dataset):,}\")\n",
    "print(f\"validation dataset size : {len(validation_dataset):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69446\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "ds_prev = \"dataset_ani2x_hard_round2_wb97m-d3bj_def2-tzvppd.pkl\"\n",
    "\n",
    "with open(ds_prev,\"rb\") as f:\n",
    "    dataset_prev = pickle.load(f)\n",
    "print(len(dataset_prev[\"training\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label = \"dataset_ani2x_hard_round2+dmp_wb97m-d3bj_def2-tzvppd.pkl\"\n",
    "with open(label, \"wb\") as f:\n",
    "    pickle.dump(\n",
    "        {\n",
    "            \"training\": [dataset[j] for j in train_idx]+dataset_prev[\"training\"]\n",
    "            ,\"validation\": [dataset[j] for j in validation_idx]+dataset_prev[\"validation\"],\n",
    "        },\n",
    "        f,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pthomas/Programs/qcAPI\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
